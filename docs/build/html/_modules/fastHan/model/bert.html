

<!DOCTYPE html>
<html class="writer-html4" lang="zh-CN" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>fastHan.model.bert &mdash; fastHan 0.5.0 文档</title>
  

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../../_static/language_data.js"></script>
        <script type="text/javascript" src="../../../_static/translations.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    
    <link rel="index" title="索引" href="../../../genindex.html" />
    <link rel="search" title="搜索" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../index.html" class="icon icon-home" alt="Documentation Home"> fastHan
          

          
          </a>

          
            
            
              <div class="version">
                0.5.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../user/example.html"> 语法样例</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">fastHan</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../../index.html">模块代码</a> &raquo;</li>
        
      <li>fastHan.model.bert</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>fastHan.model.bert 源代码</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">itertools</span> <span class="k">import</span> <span class="n">chain</span>


<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="k">import</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">fastNLP.embeddings.contextual_embedding</span> <span class="k">import</span> <span class="n">ContextualEmbedding</span>
<span class="kn">from</span> <span class="nn">fastNLP.core</span> <span class="k">import</span> <span class="n">logger</span>
<span class="kn">from</span> <span class="nn">fastNLP.core.vocabulary</span> <span class="k">import</span> <span class="n">Vocabulary</span>
<span class="kn">from</span> <span class="nn">fastNLP.io.file_utils</span> <span class="k">import</span> <span class="n">PRETRAINED_BERT_MODEL_DIR</span>
<span class="kn">from</span> <span class="nn">.old_fastNLP_bert</span> <span class="k">import</span> <span class="n">_WordPieceBertModel</span><span class="p">,</span> <span class="n">BertTokenizer</span>
<span class="kn">from</span> <span class="nn">.bert_encoder_theseus</span> <span class="k">import</span> <span class="n">BertModel</span>


<div class="viewcode-block" id="BertEmbedding"><a class="viewcode-back" href="../../../fastHan.model.bert.html#fastHan.model.bert.BertEmbedding">[文档]</a><span class="k">class</span> <span class="nc">BertEmbedding</span><span class="p">(</span><span class="n">ContextualEmbedding</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    使用BERT对words进行编码的Embedding。建议将输入的words长度限制在430以内，而不要使用512(根据预训练模型参数，可能有变化)。这是由于</span>
<span class="sd">    预训练的bert模型长度限制为512个token，而因为输入的word是未进行word piece分割的(word piece的分割有BertEmbedding在输入word</span>
<span class="sd">    时切分)，在分割之后长度可能会超过最大长度限制。</span>

<span class="sd">    BertEmbedding可以支持自动下载权重，当前支持的模型有以下的几种(待补充):</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; from fastNLP import Vocabulary</span>
<span class="sd">        &gt;&gt;&gt; from fastNLP.embeddings import BertEmbedding</span>
<span class="sd">        &gt;&gt;&gt; vocab = Vocabulary().add_word_lst(&quot;The whether is good .&quot;.split())</span>
<span class="sd">        &gt;&gt;&gt; embed = BertEmbedding(vocab, model_dir_or_name=&#39;en-base-uncased&#39;, requires_grad=False, layers=&#39;4,-2,-1&#39;)</span>
<span class="sd">        &gt;&gt;&gt; words = torch.LongTensor([[vocab.to_index(word) for word in &quot;The whether is good .&quot;.split()]])</span>
<span class="sd">        &gt;&gt;&gt; outputs = embed(words)</span>
<span class="sd">        &gt;&gt;&gt; outputs.size()</span>
<span class="sd">        &gt;&gt;&gt; # torch.Size([1, 5, 2304])</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
<div class="viewcode-block" id="BertEmbedding.__init__"><a class="viewcode-back" href="../../../fastHan.model.bert.html#fastHan.model.bert.BertEmbedding.__init__">[文档]</a>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab</span><span class="p">:</span> <span class="n">Vocabulary</span><span class="p">,</span> <span class="n">model_dir_or_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;en-base-uncased&#39;</span><span class="p">,</span> <span class="n">layers</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;-1&#39;</span><span class="p">,</span>
                 <span class="n">pool_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;first&#39;</span><span class="p">,</span> <span class="n">word_dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">include_cls_sep</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="n">pooled_cls</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">requires_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">auto_truncate</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">layer_num</span><span class="o">=</span><span class="mi">12</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        </span>
<span class="sd">        :param ~fastNLP.Vocabulary vocab: 词表</span>
<span class="sd">        :param str model_dir_or_name: 模型所在目录或者模型的名称。当传入模型所在目录时，目录中应该包含一个词表文件(以.txt作为后缀名),</span>
<span class="sd">            权重文件(以.bin作为文件后缀名), 配置文件(以.json作为后缀名)。</span>
<span class="sd">        :param str layers: 输出embedding表示来自于哪些层，不同层的结果按照layers中的顺序在最后一维concat起来。以&#39;,&#39;隔开层数，层的序号是</span>
<span class="sd">            从0开始，可以以负数去索引倒数几层。</span>
<span class="sd">        :param str pool_method: 因为在bert中，每个word会被表示为多个word pieces, 当获取一个word的表示的时候，怎样从它的word pieces</span>
<span class="sd">            中计算得到它对应的表示。支持 ``last`` , ``first`` , ``avg`` , ``max``。</span>
<span class="sd">        :param float word_dropout: 以多大的概率将一个词替换为unk。这样既可以训练unk也是一定的regularize。</span>
<span class="sd">        :param float dropout: 以多大的概率对embedding的表示进行Dropout。0.1即随机将10%的值置为0。</span>
<span class="sd">        :param bool include_cls_sep: bool，在bert计算句子的表示的时候，需要在前面加上[CLS]和[SEP], 是否在结果中保留这两个内容。 这样</span>
<span class="sd">            会使得word embedding的结果比输入的结果长两个token。如果该值为True，则在使用 :class::StackEmbedding 可能会与其它类型的</span>
<span class="sd">            embedding长度不匹配。</span>
<span class="sd">        :param bool pooled_cls: 返回的[CLS]是否使用预训练中的BertPool映射一下，仅在include_cls_sep时有效。如果下游任务只取[CLS]做预测，</span>
<span class="sd">            一般该值为True。</span>
<span class="sd">        :param bool requires_grad: 是否需要gradient以更新Bert的权重。</span>
<span class="sd">        :param bool auto_truncate: 当句子words拆分为word pieces长度超过bert最大允许长度(一般为512), 自动截掉拆分后的超过510个</span>
<span class="sd">            word pieces后的内容，并将第512个word piece置为[SEP]。超过长度的部分的encode结果直接全部置零。一般仅有只使用[CLS]</span>
<span class="sd">            来进行分类的任务将auto_truncate置为True。</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BertEmbedding</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">vocab</span><span class="p">,</span> <span class="n">word_dropout</span><span class="o">=</span><span class="n">word_dropout</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">model_dir_or_name</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">in</span> <span class="n">PRETRAINED_BERT_MODEL_DIR</span><span class="p">:</span>
            <span class="k">if</span> <span class="s1">&#39;cn&#39;</span> <span class="ow">in</span> <span class="n">model_dir_or_name</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">and</span> <span class="n">pool_method</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;first&#39;</span><span class="p">,</span> <span class="s1">&#39;last&#39;</span><span class="p">):</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;For Chinese bert, pooled_method should choose from &#39;first&#39;, &#39;last&#39; in order to achieve&quot;</span>
                               <span class="s2">&quot; faster speed.&quot;</span><span class="p">)</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;For Chinese bert, pooled_method should choose from &#39;first&#39;, &#39;last&#39; in order to achieve&quot;</span>
                              <span class="s2">&quot; faster speed.&quot;</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">_word_sep_index</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="s1">&#39;[SEP]&#39;</span> <span class="ow">in</span> <span class="n">vocab</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_word_sep_index</span> <span class="o">=</span> <span class="n">vocab</span><span class="p">[</span><span class="s1">&#39;[SEP]&#39;</span><span class="p">]</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">_WordBertModel</span><span class="p">(</span><span class="n">model_dir_or_name</span><span class="o">=</span><span class="n">model_dir_or_name</span><span class="p">,</span> <span class="n">vocab</span><span class="o">=</span><span class="n">vocab</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="n">layers</span><span class="p">,</span>
                                    <span class="n">pool_method</span><span class="o">=</span><span class="n">pool_method</span><span class="p">,</span> <span class="n">include_cls_sep</span><span class="o">=</span><span class="n">include_cls_sep</span><span class="p">,</span>
                                    <span class="n">pooled_cls</span><span class="o">=</span><span class="n">pooled_cls</span><span class="p">,</span> <span class="n">auto_truncate</span><span class="o">=</span><span class="n">auto_truncate</span><span class="p">,</span> <span class="n">min_freq</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">layer_num</span><span class="o">=</span><span class="n">layer_num</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="n">requires_grad</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_embed_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">hidden_size</span></div>
    
    <span class="k">def</span> <span class="nf">_delete_model_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span>
    
<div class="viewcode-block" id="BertEmbedding.forward"><a class="viewcode-back" href="../../../fastHan.model.bert.html#fastHan.model.bert.BertEmbedding.forward">[文档]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">words</span><span class="p">,</span><span class="n">layers</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        计算words的bert embedding表示。计算之前会在每句话的开始增加[CLS]在结束增加[SEP], 并根据include_cls_sep判断要不要</span>
<span class="sd">            删除这两个token的表示。</span>

<span class="sd">        :param torch.LongTensor words: [batch_size, max_len]</span>
<span class="sd">        :return: torch.FloatTensor. batch_size x max_len x (768*len(self.layers))</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">words</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_word</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_sent_reprs</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">outputs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">words</span><span class="p">,</span><span class="n">layers</span><span class="p">)</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="o">*</span><span class="n">outputs</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span></div>
    
<div class="viewcode-block" id="BertEmbedding.drop_word"><a class="viewcode-back" href="../../../fastHan.model.bert.html#fastHan.model.bert.BertEmbedding.drop_word">[文档]</a>    <span class="k">def</span> <span class="nf">drop_word</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">words</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        按照设定随机将words设置为unknown_index。</span>

<span class="sd">        :param torch.LongTensor words: batch_size x max_len</span>
<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">word_dropout</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_word_sep_index</span><span class="p">:</span>  <span class="c1"># 不能drop sep</span>
                    <span class="n">sep_mask</span> <span class="o">=</span> <span class="n">words</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_word_sep_index</span><span class="p">)</span>
                <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">fill_value</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">word_dropout</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">words</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bernoulli</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># dropout_word越大，越多位置为1</span>
                <span class="n">pad_mask</span> <span class="o">=</span> <span class="n">words</span><span class="o">.</span><span class="n">ne</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
                <span class="n">mask</span> <span class="o">=</span> <span class="n">pad_mask</span><span class="o">.</span><span class="fm">__and__</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span>  <span class="c1"># pad的位置不为unk</span>
                <span class="n">words</span> <span class="o">=</span> <span class="n">words</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_word_unk_index</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_word_sep_index</span><span class="p">:</span>
                    <span class="n">words</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">sep_mask</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_word_sep_index</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">words</span></div></div>


<div class="viewcode-block" id="BertWordPieceEncoder"><a class="viewcode-back" href="../../../fastHan.model.bert.html#fastHan.model.bert.BertWordPieceEncoder">[文档]</a><span class="k">class</span> <span class="nc">BertWordPieceEncoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    读取bert模型，读取之后调用index_dataset方法在dataset中生成word_pieces这一列。</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
<div class="viewcode-block" id="BertWordPieceEncoder.__init__"><a class="viewcode-back" href="../../../fastHan.model.bert.html#fastHan.model.bert.BertWordPieceEncoder.__init__">[文档]</a>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_dir_or_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;en-base-uncased&#39;</span><span class="p">,</span> <span class="n">layers</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;-1&#39;</span><span class="p">,</span> <span class="n">pooled_cls</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="n">word_dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        </span>
<span class="sd">        :param str model_dir_or_name: 模型所在目录或者模型的名称。默认值为 ``en-base-uncased``</span>
<span class="sd">        :param str layers: 最终结果中的表示。以&#39;,&#39;隔开层数，可以以负数去索引倒数几层</span>
<span class="sd">        :param bool pooled_cls: 返回的句子开头的[CLS]是否使用预训练中的BertPool映射一下，仅在include_cls_sep时有效。如果下游任务只取</span>
<span class="sd">            [CLS]做预测，一般该值为True。</span>
<span class="sd">        :param float word_dropout: 以多大的概率将一个词替换为unk。这样既可以训练unk也是一定的regularize。</span>
<span class="sd">        :param float dropout: 以多大的概率对embedding的表示进行Dropout。0.1即随机将10%的值置为0。</span>
<span class="sd">        :param bool requires_grad: 是否需要gradient。</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">_WordPieceBertModel</span><span class="p">(</span><span class="n">model_dir_or_name</span><span class="o">=</span><span class="n">model_dir_or_name</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="n">layers</span><span class="p">,</span> <span class="n">pooled_cls</span><span class="o">=</span><span class="n">pooled_cls</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sep_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_sep_index</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_wordpiece_pad_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_wordpiece_pad_index</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_wordpiece_unk_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">_wordpiece_unknown_index</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_embed_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="n">requires_grad</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word_dropout</span> <span class="o">=</span> <span class="n">word_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span></div>
    
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">embed_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_embed_size</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">embedding_dim</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_embed_size</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">num_embedding</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span>
    
<div class="viewcode-block" id="BertWordPieceEncoder.index_datasets"><a class="viewcode-back" href="../../../fastHan.model.bert.html#fastHan.model.bert.BertWordPieceEncoder.index_datasets">[文档]</a>    <span class="k">def</span> <span class="nf">index_datasets</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">datasets</span><span class="p">,</span> <span class="n">field_name</span><span class="p">,</span> <span class="n">add_cls_sep</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        使用bert的tokenizer新生成word_pieces列加入到datasets中，并将他们设置为input,且将word_pieces这一列的pad value设置为了</span>
<span class="sd">        bert的pad value。</span>

<span class="sd">        :param ~fastNLP.DataSet datasets: DataSet对象</span>
<span class="sd">        :param str field_name: 基于哪一列的内容生成word_pieces列。这一列中每个数据应该是List[str]的形式。</span>
<span class="sd">        :param bool add_cls_sep: 如果首尾不是[CLS]与[SEP]会在首尾额外加入[CLS]与[SEP]。</span>
<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">index_dataset</span><span class="p">(</span><span class="o">*</span><span class="n">datasets</span><span class="p">,</span> <span class="n">field_name</span><span class="o">=</span><span class="n">field_name</span><span class="p">,</span> <span class="n">add_cls_sep</span><span class="o">=</span><span class="n">add_cls_sep</span><span class="p">)</span></div>
    
<div class="viewcode-block" id="BertWordPieceEncoder.forward"><a class="viewcode-back" href="../../../fastHan.model.bert.html#fastHan.model.bert.BertWordPieceEncoder.forward">[文档]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">word_pieces</span><span class="p">,</span> <span class="n">token_type_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        计算words的bert embedding表示。传入的words中应该自行包含[CLS]与[SEP]的tag。</span>

<span class="sd">        :param words: batch_size x max_len</span>
<span class="sd">        :param token_type_ids: batch_size x max_len, 用于区分前一句和后一句话. 如果不传入，则自动生成(大部分情况，都不需要输入),</span>
<span class="sd">            第一个[SEP]及之前为0, 第二个[SEP]及到第一个[SEP]之间为1; 第三个[SEP]及到第二个[SEP]之间为0，依次往后推。</span>
<span class="sd">        :return: torch.FloatTensor. batch_size x max_len x (768*len(self.layers))</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">sep_mask</span> <span class="o">=</span> <span class="n">word_pieces</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_sep_index</span><span class="p">)</span>  <span class="c1"># batch_size x max_len</span>
            <span class="k">if</span> <span class="n">token_type_ids</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">sep_mask_cumsum</span> <span class="o">=</span> <span class="n">sep_mask</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">dims</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">dims</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
                <span class="n">token_type_ids</span> <span class="o">=</span> <span class="n">sep_mask_cumsum</span><span class="o">.</span><span class="n">fmod</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">token_type_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">():</span>  <span class="c1"># 如果开头是奇数，则需要flip一下结果，因为需要保证开头为0</span>
                    <span class="n">token_type_ids</span> <span class="o">=</span> <span class="n">token_type_ids</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>
        
        <span class="n">word_pieces</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_word</span><span class="p">(</span><span class="n">word_pieces</span><span class="p">)</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">word_pieces</span><span class="p">,</span> <span class="n">token_type_ids</span><span class="p">)</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="o">*</span><span class="n">outputs</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout_layer</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span></div>
    
<div class="viewcode-block" id="BertWordPieceEncoder.drop_word"><a class="viewcode-back" href="../../../fastHan.model.bert.html#fastHan.model.bert.BertWordPieceEncoder.drop_word">[文档]</a>    <span class="k">def</span> <span class="nf">drop_word</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">words</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        按照设定随机将words设置为unknown_index。</span>

<span class="sd">        :param torch.LongTensor words: batch_size x max_len</span>
<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">word_dropout</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_word_sep_index</span><span class="p">:</span>  <span class="c1"># 不能drop sep</span>
                    <span class="n">sep_mask</span> <span class="o">=</span> <span class="n">words</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_wordpiece_unk_index</span><span class="p">)</span>
                <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">fill_value</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">word_dropout</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">words</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bernoulli</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># dropout_word越大，越多位置为1</span>
                <span class="n">pad_mask</span> <span class="o">=</span> <span class="n">words</span><span class="o">.</span><span class="n">ne</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_wordpiece_pad_index</span><span class="p">)</span>
                <span class="n">mask</span> <span class="o">=</span> <span class="n">pad_mask</span><span class="o">.</span><span class="fm">__and__</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span>  <span class="c1"># pad的位置不为unk</span>
                <span class="n">words</span> <span class="o">=</span> <span class="n">words</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_word_unk_index</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_word_sep_index</span><span class="p">:</span>
                    <span class="n">words</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">sep_mask</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_wordpiece_unk_index</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">words</span></div></div>


<span class="k">class</span> <span class="nc">_WordBertModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_dir_or_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">vocab</span><span class="p">:</span> <span class="n">Vocabulary</span><span class="p">,</span> <span class="n">layers</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;-1&#39;</span><span class="p">,</span> <span class="n">pool_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;first&#39;</span><span class="p">,</span>
                 <span class="n">include_cls_sep</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">pooled_cls</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">auto_truncate</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">min_freq</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">layer_num</span><span class="o">=</span><span class="mi">12</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenzier</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_dir_or_name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_dir_or_name</span><span class="p">,</span><span class="n">layer_num</span><span class="o">=</span><span class="n">layer_num</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_max_position_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">max_position_embeddings</span>
        <span class="c1">#  检查encoder_layer_number是否合理</span>
        <span class="n">encoder_layer_number</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">layer</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">layers</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">)))</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">layer</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">assert</span> <span class="o">-</span><span class="n">layer</span> <span class="o">&lt;=</span> <span class="n">encoder_layer_number</span><span class="p">,</span> <span class="n">f</span><span class="s2">&quot;The layer index:</span><span class="si">{layer}</span><span class="s2"> is out of scope for &quot;</span> \
                                                       <span class="n">f</span><span class="s2">&quot;a bert model with </span><span class="si">{encoder_layer_number}</span><span class="s2"> layers.&quot;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">assert</span> <span class="n">layer</span> <span class="o">&lt;</span> <span class="n">encoder_layer_number</span><span class="p">,</span> <span class="n">f</span><span class="s2">&quot;The layer index:</span><span class="si">{layer}</span><span class="s2"> is out of scope for &quot;</span> \
                                                     <span class="n">f</span><span class="s2">&quot;a bert model with </span><span class="si">{encoder_layer_number}</span><span class="s2"> layers.&quot;</span>
        
        <span class="k">assert</span> <span class="n">pool_method</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;avg&#39;</span><span class="p">,</span> <span class="s1">&#39;max&#39;</span><span class="p">,</span> <span class="s1">&#39;first&#39;</span><span class="p">,</span> <span class="s1">&#39;last&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pool_method</span> <span class="o">=</span> <span class="n">pool_method</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">include_cls_sep</span> <span class="o">=</span> <span class="n">include_cls_sep</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pooled_cls</span> <span class="o">=</span> <span class="n">pooled_cls</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">auto_truncate</span> <span class="o">=</span> <span class="n">auto_truncate</span>
        
        <span class="c1"># 将所有vocab中word的wordpiece计算出来, 需要额外考虑[CLS]和[SEP]</span>
        <span class="k">if</span> <span class="n">model_dir_or_name</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">in</span> <span class="n">PRETRAINED_BERT_MODEL_DIR</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Start to generate word pieces for word.&quot;</span><span class="p">)</span>
        <span class="c1"># 第一步统计出需要的word_piece, 然后创建新的embed和word_piece_vocab, 然后填入值</span>
        <span class="n">word_piece_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;[CLS]&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;[SEP]&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span>  <span class="c1"># 用到的word_piece以及新增的</span>
        <span class="n">found_count</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_has_sep_in_vocab</span> <span class="o">=</span> <span class="s1">&#39;[SEP]&#39;</span> <span class="ow">in</span> <span class="n">vocab</span>  <span class="c1"># 用来判断传入的数据是否需要生成token_ids</span>
        <span class="k">if</span> <span class="s1">&#39;[sep]&#39;</span> <span class="ow">in</span> <span class="n">vocab</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Lower cased [sep] detected, it cannot be correctly recognized as [SEP] by BertEmbedding.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="s2">&quot;[CLS]&quot;</span> <span class="ow">in</span> <span class="n">vocab</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;[CLS] detected in your vocabulary. BertEmbedding will add [CSL] and [SEP] to the begin &quot;</span>
                          <span class="s2">&quot;and end of the input automatically, make sure you don&#39;t add [CLS] and [SEP] at the begin&quot;</span>
                          <span class="s2">&quot; and end.&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">vocab</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">index</span> <span class="o">==</span> <span class="n">vocab</span><span class="o">.</span><span class="n">padding_idx</span><span class="p">:</span>  <span class="c1"># pad是个特殊的符号</span>
                <span class="n">word</span> <span class="o">=</span> <span class="s1">&#39;[PAD]&#39;</span>
            <span class="k">elif</span> <span class="n">index</span> <span class="o">==</span> <span class="n">vocab</span><span class="o">.</span><span class="n">unknown_idx</span><span class="p">:</span>
                <span class="n">word</span> <span class="o">=</span> <span class="s1">&#39;[UNK]&#39;</span>
            <span class="n">word_pieces</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenzier</span><span class="o">.</span><span class="n">wordpiece_tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_pieces</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">vocab</span><span class="o">.</span><span class="n">_is_word_no_create_entry</span><span class="p">(</span><span class="n">word</span><span class="p">):</span>  <span class="c1"># 如果是train中的值, 但是却没有找到</span>
                    <span class="k">if</span> <span class="n">index</span> <span class="o">!=</span> <span class="n">vocab</span><span class="o">.</span><span class="n">unknown_idx</span> <span class="ow">and</span> <span class="n">word_pieces</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;[UNK]&#39;</span><span class="p">:</span>  <span class="c1"># 说明这个词不在原始的word里面</span>
                        <span class="k">if</span> <span class="n">vocab</span><span class="o">.</span><span class="n">word_count</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">min_freq</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">vocab</span><span class="o">.</span><span class="n">_is_word_no_create_entry</span><span class="p">(</span>
                                <span class="n">word</span><span class="p">):</span>  <span class="c1"># 出现次数大于这个次数才新增</span>
                            <span class="n">word_piece_dict</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># 新增一个值</span>
                        <span class="k">continue</span>
            <span class="k">for</span> <span class="n">word_piece</span> <span class="ow">in</span> <span class="n">word_pieces</span><span class="p">:</span>
                <span class="n">word_piece_dict</span><span class="p">[</span><span class="n">word_piece</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="n">found_count</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">original_embed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">word_embeddings</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span>
        <span class="c1"># 特殊词汇要特殊处理</span>
        <span class="n">embed</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">word_piece_dict</span><span class="p">),</span> <span class="n">original_embed</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># 新的embed</span>
        <span class="n">new_word_piece_vocab</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">OrderedDict</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="s1">&#39;[PAD]&#39;</span><span class="p">,</span> <span class="s1">&#39;[UNK]&#39;</span><span class="p">]):</span>
            <span class="n">word_piece_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="n">embed</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">original_embed</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenzier</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">token</span><span class="p">]]</span>
            <span class="n">new_word_piece_vocab</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="o">=</span> <span class="n">index</span>
        <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">word_piece_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">token</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenzier</span><span class="o">.</span><span class="n">vocab</span><span class="p">:</span>
                <span class="n">embed</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">new_word_piece_vocab</span><span class="p">)]</span> <span class="o">=</span> <span class="n">original_embed</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenzier</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">token</span><span class="p">]]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">embed</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">new_word_piece_vocab</span><span class="p">)]</span> <span class="o">=</span> <span class="n">original_embed</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenzier</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="s1">&#39;[UNK]&#39;</span><span class="p">]]</span>
            <span class="n">new_word_piece_vocab</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">new_word_piece_vocab</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenzier</span><span class="o">.</span><span class="n">_reinit_on_new_vocab</span><span class="p">(</span><span class="n">new_word_piece_vocab</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">word_embeddings</span> <span class="o">=</span> <span class="n">embed</span>
        
        <span class="n">word_to_wordpieces</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">word_pieces_lengths</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">vocab</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">index</span> <span class="o">==</span> <span class="n">vocab</span><span class="o">.</span><span class="n">padding_idx</span><span class="p">:</span>  <span class="c1"># pad是个特殊的符号</span>
                <span class="n">word</span> <span class="o">=</span> <span class="s1">&#39;[PAD]&#39;</span>
            <span class="k">elif</span> <span class="n">index</span> <span class="o">==</span> <span class="n">vocab</span><span class="o">.</span><span class="n">unknown_idx</span><span class="p">:</span>
                <span class="n">word</span> <span class="o">=</span> <span class="s1">&#39;[UNK]&#39;</span>
            <span class="n">word_pieces</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenzier</span><span class="o">.</span><span class="n">wordpiece_tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
            <span class="n">word_pieces</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenzier</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">word_pieces</span><span class="p">)</span>
            <span class="n">word_to_wordpieces</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word_pieces</span><span class="p">)</span>
            <span class="n">word_pieces_lengths</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">word_pieces</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cls_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenzier</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="s1">&#39;[CLS]&#39;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sep_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenzier</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="s1">&#39;[SEP]&#39;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_word_pad_index</span> <span class="o">=</span> <span class="n">vocab</span><span class="o">.</span><span class="n">padding_idx</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_wordpiece_pad_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenzier</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="s1">&#39;[PAD]&#39;</span><span class="p">]</span>  <span class="c1"># 需要用于生成word_piece</span>
        <span class="k">if</span> <span class="n">model_dir_or_name</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">in</span> <span class="n">PRETRAINED_BERT_MODEL_DIR</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Found(Or segment into word pieces) </span><span class="si">{}</span><span class="s2"> words out of </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">found_count</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word_to_wordpieces</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">word_to_wordpieces</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;word_pieces_lengths&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">word_pieces_lengths</span><span class="p">))</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;Successfully generate word pieces.&quot;</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">words</span><span class="p">,</span><span class="n">layers</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        :param words: torch.LongTensor, batch_size x max_len</span>
<span class="sd">        :return: num_layers x batch_size x max_len x hidden_size或者num_layers x batch_size x (max_len+2) x hidden_size</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">origin_layer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span>
        <span class="k">if</span> <span class="n">layers</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">layers</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">)))</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">max_word_len</span> <span class="o">=</span> <span class="n">words</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
            <span class="n">word_mask</span> <span class="o">=</span> <span class="n">words</span><span class="o">.</span><span class="n">ne</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_word_pad_index</span><span class="p">)</span>  <span class="c1"># 为1的地方有word</span>
            <span class="n">seq_len</span> <span class="o">=</span> <span class="n">word_mask</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">batch_word_pieces_length</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">word_pieces_lengths</span><span class="p">[</span><span class="n">words</span><span class="p">]</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">word_mask</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
                                                                                   <span class="mi">0</span><span class="p">)</span>  <span class="c1"># batch_size x max_len</span>
            <span class="n">word_pieces_lengths</span> <span class="o">=</span> <span class="n">batch_word_pieces_length</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># batch_size</span>
            <span class="n">word_piece_length</span> <span class="o">=</span> <span class="n">batch_word_pieces_length</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>  <span class="c1"># 表示word piece的长度(包括padding)</span>
            <span class="k">if</span> <span class="n">word_piece_length</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_position_embeddings</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">auto_truncate</span><span class="p">:</span>
                    <span class="n">word_pieces_lengths</span> <span class="o">=</span> <span class="n">word_pieces_lengths</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span>
                        <span class="n">word_pieces_lengths</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_position_embeddings</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_max_position_embeddings</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                        <span class="s2">&quot;After split words into word pieces, the lengths of word pieces are longer than the &quot;</span>
                        <span class="n">f</span><span class="s2">&quot;maximum allowed sequence length:</span><span class="si">{self._max_position_embeddings}</span><span class="s2"> of bert. You can set &quot;</span>
                        <span class="n">f</span><span class="s2">&quot;`auto_truncate=True` for BertEmbedding to automatically truncate overlong input.&quot;</span><span class="p">)</span>
            
            <span class="c1"># +2是由于需要加入[CLS]与[SEP]</span>
            <span class="n">word_pieces</span> <span class="o">=</span> <span class="n">words</span><span class="o">.</span><span class="n">new_full</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="n">word_piece_length</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_position_embeddings</span><span class="p">)),</span>
                                         <span class="n">fill_value</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_wordpiece_pad_index</span><span class="p">)</span>
            <span class="n">attn_masks</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">word_pieces</span><span class="p">)</span>
            <span class="c1"># 1. 获取words的word_pieces的id，以及对应的span范围</span>
            <span class="n">word_indexes</span> <span class="o">=</span> <span class="n">words</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
                <span class="n">word_pieces_i</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">chain</span><span class="p">(</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">word_to_wordpieces</span><span class="p">[</span><span class="n">word_indexes</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:</span><span class="n">seq_len</span><span class="p">[</span><span class="n">i</span><span class="p">]]]))</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">auto_truncate</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_pieces_i</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_position_embeddings</span> <span class="o">-</span> <span class="mi">2</span><span class="p">:</span>
                    <span class="n">word_pieces_i</span> <span class="o">=</span> <span class="n">word_pieces_i</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">_max_position_embeddings</span> <span class="o">-</span> <span class="mi">2</span><span class="p">]</span>
                <span class="n">word_pieces</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span><span class="n">word_pieces_lengths</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">word_pieces_i</span><span class="p">)</span>
                <span class="n">attn_masks</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:</span><span class="n">word_pieces_lengths</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># 添加[cls]和[sep]</span>
            <span class="n">word_pieces</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cls_index</span><span class="p">)</span>
            <span class="n">batch_indexes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
            <span class="n">word_pieces</span><span class="p">[</span><span class="n">batch_indexes</span><span class="p">,</span> <span class="n">word_pieces_lengths</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sep_index</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_has_sep_in_vocab</span><span class="p">:</span>  <span class="c1"># 但[SEP]在vocab中出现应该才会需要token_ids</span>
                <span class="n">sep_mask</span> <span class="o">=</span> <span class="n">word_pieces</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_sep_index</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>  <span class="c1"># batch_size x max_len</span>
                <span class="n">sep_mask_cumsum</span> <span class="o">=</span> <span class="n">sep_mask</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">dims</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">dims</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
                <span class="n">token_type_ids</span> <span class="o">=</span> <span class="n">sep_mask_cumsum</span><span class="o">.</span><span class="n">fmod</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">token_type_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">():</span>  <span class="c1"># 如果开头是奇数，则需要flip一下结果，因为需要保证开头为0</span>
                    <span class="n">token_type_ids</span> <span class="o">=</span> <span class="n">token_type_ids</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">token_type_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">word_pieces</span><span class="p">)</span>
        <span class="c1"># 2. 获取hidden的结果，根据word_pieces进行对应的pool计算</span>
        <span class="c1"># all_outputs: [batch_size x max_len x hidden_size, batch_size x max_len x hidden_size, ...]</span>
        <span class="n">bert_outputs</span><span class="p">,</span> <span class="n">pooled_cls</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">word_pieces</span><span class="p">,</span> <span class="n">token_type_ids</span><span class="o">=</span><span class="n">token_type_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attn_masks</span><span class="p">,</span>
                                                <span class="n">output_all_encoded_layers</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="c1"># output_layers = [self.layers]  # len(self.layers) x batch_size x real_word_piece_length x hidden_size</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">include_cls_sep</span><span class="p">:</span>
            <span class="n">s_shift</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">bert_outputs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">new_zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">max_word_len</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span>
                                                     <span class="n">bert_outputs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="n">s_shift</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">bert_outputs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">new_zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">max_word_len</span><span class="p">,</span>
                                                 <span class="n">bert_outputs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">batch_word_pieces_cum_length</span> <span class="o">=</span> <span class="n">batch_word_pieces_length</span><span class="o">.</span><span class="n">new_zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">max_word_len</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">batch_word_pieces_cum_length</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span> <span class="o">=</span> <span class="n">batch_word_pieces_length</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># batch_size x max_len</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool_method</span> <span class="o">==</span> <span class="s1">&#39;first&#39;</span><span class="p">:</span>
            <span class="n">batch_word_pieces_cum_length</span> <span class="o">=</span> <span class="n">batch_word_pieces_cum_length</span><span class="p">[:,</span> <span class="p">:</span><span class="n">seq_len</span><span class="o">.</span><span class="n">max</span><span class="p">()]</span>
            <span class="n">batch_word_pieces_cum_length</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">batch_word_pieces_cum_length</span><span class="o">.</span><span class="n">ge</span><span class="p">(</span><span class="n">word_piece_length</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
            <span class="n">_batch_indexes</span> <span class="o">=</span> <span class="n">batch_indexes</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">expand</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">batch_word_pieces_cum_length</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)))</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool_method</span> <span class="o">==</span> <span class="s1">&#39;last&#39;</span><span class="p">:</span>
            <span class="n">batch_word_pieces_cum_length</span> <span class="o">=</span> <span class="n">batch_word_pieces_cum_length</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:</span><span class="n">seq_len</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span>
            <span class="n">batch_word_pieces_cum_length</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">batch_word_pieces_cum_length</span><span class="o">.</span><span class="n">ge</span><span class="p">(</span><span class="n">word_piece_length</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
            <span class="n">_batch_indexes</span> <span class="o">=</span> <span class="n">batch_indexes</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">expand</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">batch_word_pieces_cum_length</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)))</span>

        <span class="k">for</span> <span class="n">l_index</span><span class="p">,</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">):</span>
            <span class="n">output_layer</span> <span class="o">=</span> <span class="n">bert_outputs</span><span class="p">[</span><span class="n">l</span><span class="p">]</span>
            <span class="n">real_word_piece_length</span> <span class="o">=</span> <span class="n">output_layer</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span>
            <span class="k">if</span> <span class="n">word_piece_length</span> <span class="o">&gt;</span> <span class="n">real_word_piece_length</span><span class="p">:</span>  <span class="c1"># 如果实际上是截取出来的</span>
                <span class="n">paddings</span> <span class="o">=</span> <span class="n">output_layer</span><span class="o">.</span><span class="n">new_zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span>
                                                  <span class="n">word_piece_length</span> <span class="o">-</span> <span class="n">real_word_piece_length</span><span class="p">,</span>
                                                  <span class="n">output_layer</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
                <span class="n">output_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">output_layer</span><span class="p">,</span> <span class="n">paddings</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
            <span class="c1"># 从word_piece collapse到word的表示</span>
            <span class="n">truncate_output_layer</span> <span class="o">=</span> <span class="n">output_layer</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># 删除[CLS]与[SEP] batch_size x len x hidden_size</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool_method</span> <span class="o">==</span> <span class="s1">&#39;first&#39;</span><span class="p">:</span>
                <span class="n">tmp</span> <span class="o">=</span> <span class="n">truncate_output_layer</span><span class="p">[</span><span class="n">_batch_indexes</span><span class="p">,</span> <span class="n">batch_word_pieces_cum_length</span><span class="p">]</span>
                <span class="n">tmp</span> <span class="o">=</span> <span class="n">tmp</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">word_mask</span><span class="p">[:,</span> <span class="p">:</span><span class="n">batch_word_pieces_cum_length</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
                <span class="n">outputs</span><span class="p">[</span><span class="n">l_index</span><span class="p">,</span> <span class="p">:,</span> <span class="n">s_shift</span><span class="p">:</span><span class="n">batch_word_pieces_cum_length</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">+</span><span class="n">s_shift</span><span class="p">]</span> <span class="o">=</span> <span class="n">tmp</span>

            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool_method</span> <span class="o">==</span> <span class="s1">&#39;last&#39;</span><span class="p">:</span>
                <span class="n">tmp</span> <span class="o">=</span> <span class="n">truncate_output_layer</span><span class="p">[</span><span class="n">_batch_indexes</span><span class="p">,</span> <span class="n">batch_word_pieces_cum_length</span><span class="p">]</span>
                <span class="n">tmp</span> <span class="o">=</span> <span class="n">tmp</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">word_mask</span><span class="p">[:,</span> <span class="p">:</span><span class="n">batch_word_pieces_cum_length</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
                <span class="n">outputs</span><span class="p">[</span><span class="n">l_index</span><span class="p">,</span> <span class="p">:,</span> <span class="n">s_shift</span><span class="p">:</span><span class="n">batch_word_pieces_cum_length</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">+</span><span class="n">s_shift</span><span class="p">]</span> <span class="o">=</span> <span class="n">tmp</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool_method</span> <span class="o">==</span> <span class="s1">&#39;max&#39;</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
                    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">seq_len</span><span class="p">[</span><span class="n">i</span><span class="p">]):</span>
                        <span class="n">start</span><span class="p">,</span> <span class="n">end</span> <span class="o">=</span> <span class="n">batch_word_pieces_cum_length</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">batch_word_pieces_cum_length</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
                        <span class="n">outputs</span><span class="p">[</span><span class="n">l_index</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">+</span> <span class="n">s_shift</span><span class="p">],</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">truncate_output_layer</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
                    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">seq_len</span><span class="p">[</span><span class="n">i</span><span class="p">]):</span>
                        <span class="n">start</span><span class="p">,</span> <span class="n">end</span> <span class="o">=</span> <span class="n">batch_word_pieces_cum_length</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">batch_word_pieces_cum_length</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
                        <span class="n">outputs</span><span class="p">[</span><span class="n">l_index</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">+</span> <span class="n">s_shift</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">truncate_output_layer</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">include_cls_sep</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">l</span> <span class="ow">in</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">bert_outputs</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">pooled_cls</span><span class="p">:</span>
                    <span class="n">outputs</span><span class="p">[</span><span class="n">l_index</span><span class="p">,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">pooled_cls</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">outputs</span><span class="p">[</span><span class="n">l_index</span><span class="p">,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">output_layer</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
                <span class="n">outputs</span><span class="p">[</span><span class="n">l_index</span><span class="p">,</span> <span class="n">batch_indexes</span><span class="p">,</span> <span class="n">seq_len</span> <span class="o">+</span> <span class="n">s_shift</span><span class="p">]</span> <span class="o">=</span> <span class="n">output_layer</span><span class="p">[</span><span class="n">batch_indexes</span><span class="p">,</span> <span class="n">seq_len</span> <span class="o">+</span> <span class="n">s_shift</span><span class="p">]</span>


        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="o">=</span><span class="n">origin_layer</span>
        <span class="c1"># 3. 最终的embedding结果</span>
        <span class="k">return</span> <span class="n">outputs</span>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; 版权所有 2020, fastHan

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>